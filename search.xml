<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>æ³¨æ„åŠ›SE CBAM CA ECAç­‰æ¨¡å—æ•´ç†</title>
      <link href="/2022/05/03/attention/"/>
      <url>/2022/05/03/attention/</url>
      
        <content type="html"><![CDATA[<h1 id="æ€»ç»“æ›¾ç»ä½¿ç”¨è¿‡çš„ä¸€äº›å³æ’å³ç”¨çš„æ¨¡å—ä»¥åŠä¸€äº›æ³¨æ„åŠ›æœºåˆ¶"><a href="#æ€»ç»“æ›¾ç»ä½¿ç”¨è¿‡çš„ä¸€äº›å³æ’å³ç”¨çš„æ¨¡å—ä»¥åŠä¸€äº›æ³¨æ„åŠ›æœºåˆ¶" class="headerlink" title="æ€»ç»“æ›¾ç»ä½¿ç”¨è¿‡çš„ä¸€äº›å³æ’å³ç”¨çš„æ¨¡å—ä»¥åŠä¸€äº›æ³¨æ„åŠ›æœºåˆ¶"></a>æ€»ç»“æ›¾ç»ä½¿ç”¨è¿‡çš„ä¸€äº›å³æ’å³ç”¨çš„æ¨¡å—ä»¥åŠä¸€äº›æ³¨æ„åŠ›æœºåˆ¶</h1><p>**</p><h2 id="æ³¨æ„åŠ›æ¨¡å—ï¼šSE"><a href="#æ³¨æ„åŠ›æ¨¡å—ï¼šSE" class="headerlink" title="æ³¨æ„åŠ›æ¨¡å—ï¼šSE"></a>æ³¨æ„åŠ›æ¨¡å—ï¼šSE</h2><p>**<br>ä»£ç æºè‡ªè¿™ä½å¤§ä½¬çš„ä»“åº“ï¼š<a href="https://github.com/moskomule/senet.pytorch">https://github.com/moskomule/senet.pytorch</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SELayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SELayer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(channel, channel // reduction, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(channel // reduction, channel, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.avg_pool(x).view(b, c)</span><br><span class="line">        y = self.fc(y).view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x * y.expand_as(x)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>SEæ¨¡å—ç†è§£èµ·æ¥æ¯”è¾ƒç®€å•ï¼Œæ€»ä½“çš„æ€æƒ³æ˜¯<strong>ç»™æ¯ä¸ªç‰¹å¾å›¾ä¸åŒçš„æƒé‡</strong>ï¼Œå…³æ³¨æ›´æœ‰ç”¨çš„ç‰¹å¾<br><img src="https://img-blog.csdnimg.cn/2fa44d6a35d04b9684ca36c0a8b65d08.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>å…·ä½“åšæ³•<br>å…ˆå¯¹è¾“å…¥çš„ç‰¹å¾å›¾è¿›è¡Œå…¨å±€æ± åŒ–ï¼Œå°†ç‰¹å¾å›¾å˜æˆ1Ã—1Ã—é€šé“æ•°ï¼Œç„¶åå…¨è¿æ¥å±‚å’Œæ¿€æ´»å‡½æ•°ï¼Œå¯¹1Ã—1Ã—é€šé“æ•°çš„ç‰¹å¾å›¾è¿›è¡Œè°ƒæ•´ï¼Œå˜æˆæ¯ä¸€ä¸ªç‰¹å¾å›¾çš„æƒé‡ï¼Œç„¶åä¸è¾“å…¥çš„ç‰¹å¾è¿›è¡Œç›¸ä¹˜ã€‚<br>ç¼ºç‚¹ï¼šæ²¡æœ‰è€ƒè™‘ç©ºé—´ä½ç½®</p><p>SEæ¨¡å—çš„æ’å…¥ä½ç½®<br>é€šè¿‡Resnetçš„åŸºç¡€æ¨¡å—å’Œbottleneckæ¨¡å— å¯ä»¥çœ‹å‡ºSEæ¨¡å—æ’å…¥åˆ°ï¼Œè·³è¿ç»“æ„addä¹‹å‰ï¼Œå¯¹å‰é¢ç‰¹å¾æå–ä¹‹åçš„ç‰¹å¾å›¾ç»™ä¸ä¸åŒçš„æƒé‡ï¼Œå†ä¸shortcutè·³è¿åˆ†æ”¯ç›¸åŠ </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SEBasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 base_width=<span class="number">64</span>, dilation=<span class="number">1</span>, norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 *, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SEBasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes, <span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.se = SELayer(planes, reduction)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.se(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SEBottleneck</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 base_width=<span class="number">64</span>, dilation=<span class="number">1</span>, norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 *, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SEBottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                               padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv3 = nn.Conv2d(planes, planes * <span class="number">4</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(planes * <span class="number">4</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.se = SELayer(planes * <span class="number">4</span>, reduction)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line">        out = self.se(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><h2 id="CBAM"><a href="#CBAM" class="headerlink" title="CBAM"></a>CBAM</h2><p>CBAMæ˜¯è§£å†³seåªè€ƒè™‘é€šé“è€Œå¿½ç•¥ç©ºé—´ä¿¡æ¯çš„å¼Šç«¯ï¼Œæå‡ºçš„ç»“æ„ï¼Œé€šè¿‡ä¸‹é¢çš„å›¾å¾ˆæ¸…æ™°çš„ç»™å‡ºäº†è¯¥æ³¨æ„åŠ›æ¨¡å—çš„ç»“æ„ï¼Œå…ˆæ˜¯ç±»å‹ä¸seçš„ç»“æ„ï¼Œäº§ç”Ÿä¸åŒçš„é€šé“æƒé‡ï¼Œä¹Ÿå°±æ˜¯ä¸åŒé€šé“çš„é‡è¦ç¨‹åº¦ã€‚<br>ç„¶åå°†æ‰€æœ‰çš„ç‰¹å¾å›¾å‹ç¼©åˆ°ä¸€ä¸ªç‰¹å¾å›¾ï¼Œæ±‚ç©ºé—´ç‰¹å¾çš„æƒé‡ï¼Œå¾ˆå®¹æ˜“ç†è§£<br><img src="https://img-blog.csdnimg.cn/34c2738eb2ee4e45a181d40d766dfd59.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br><img src="https://img-blog.csdnimg.cn/2272ca7990ee4ee893b348a6d09d799a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>ä»£ç æ¥è‡ª:<a href="https://github.com/Jongchan/attention-module">https://github.com/Jongchan/attention-module</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment">#åŸºç¡€çš„å·ç§¯æ¨¡å— ç”±å·ç§¯å±‚+BN+æ¿€æ´»å‡½æ•°</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_planes, out_planes, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, relu=<span class="literal">True</span>, bn=<span class="literal">True</span>, bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicConv, self).__init__()</span><br><span class="line">        self.out_channels = out_planes</span><br><span class="line">        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)</span><br><span class="line">        self.bn = nn.BatchNorm2d(out_planes,eps=<span class="number">1e-5</span>, momentum=<span class="number">0.01</span>, affine=<span class="literal">True</span>) <span class="keyword">if</span> bn <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.relu = nn.ReLU() <span class="keyword">if</span> relu <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">if</span> self.bn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.bn(x)</span><br><span class="line">        <span class="keyword">if</span> self.relu <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment">#å±•å¹³å±‚</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Flatten</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line"><span class="comment">#é€šé“æ³¨æ„</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelGate</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, gate_channels, reduction_ratio=<span class="number">16</span>, pool_types=[<span class="string">'avg'</span>, <span class="string">'max'</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(ChannelGate, self).__init__()</span><br><span class="line">        self.gate_channels = gate_channels</span><br><span class="line">        self.mlp = nn.Sequential(</span><br><span class="line">            Flatten(),</span><br><span class="line">            nn.Linear(gate_channels, gate_channels // reduction_ratio),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(gate_channels // reduction_ratio, gate_channels)</span><br><span class="line">            )</span><br><span class="line">        self.pool_types = pool_types</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        channel_att_sum = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> pool_type <span class="keyword">in</span> self.pool_types:</span><br><span class="line">            <span class="keyword">if</span> pool_type==<span class="string">'avg'</span>:</span><br><span class="line">                avg_pool = F.avg_pool2d( x, (x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)), stride=(x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)))</span><br><span class="line">                channel_att_raw = self.mlp( avg_pool )</span><br><span class="line">            <span class="keyword">elif</span> pool_type==<span class="string">'max'</span>:</span><br><span class="line">                max_pool = F.max_pool2d( x, (x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)), stride=(x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)))</span><br><span class="line">                channel_att_raw = self.mlp( max_pool )</span><br><span class="line">            <span class="keyword">elif</span> pool_type==<span class="string">'lp'</span>:</span><br><span class="line">                lp_pool = F.lp_pool2d( x, <span class="number">2</span>, (x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)), stride=(x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)))</span><br><span class="line">                channel_att_raw = self.mlp( lp_pool )</span><br><span class="line">            <span class="keyword">elif</span> pool_type==<span class="string">'lse'</span>:</span><br><span class="line">                <span class="comment"># LSE pool only</span></span><br><span class="line">                lse_pool = logsumexp_2d(x)</span><br><span class="line">                channel_att_raw = self.mlp( lse_pool )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> channel_att_sum <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                channel_att_sum = channel_att_raw</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                channel_att_sum = channel_att_sum + channel_att_raw</span><br><span class="line"></span><br><span class="line">        scale = F.sigmoid( channel_att_sum ).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>).expand_as(x)</span><br><span class="line">        <span class="keyword">return</span> x * scale</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logsumexp_2d</span>(<span class="params">tensor</span>):</span><br><span class="line">    tensor_flatten = tensor.view(tensor.size(<span class="number">0</span>), tensor.size(<span class="number">1</span>), -<span class="number">1</span>)</span><br><span class="line">    s, _ = torch.<span class="built_in">max</span>(tensor_flatten, dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    outputs = s + (tensor_flatten - s).exp().<span class="built_in">sum</span>(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).log()</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelPool</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.cat( (torch.<span class="built_in">max</span>(x,<span class="number">1</span>)[<span class="number">0</span>].unsqueeze(<span class="number">1</span>), torch.mean(x,<span class="number">1</span>).unsqueeze(<span class="number">1</span>)), dim=<span class="number">1</span> )</span><br><span class="line"><span class="comment">#ç©ºé—´æ³¨æ„åŠ›éƒ¨åˆ†</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialGate</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SpatialGate, self).__init__()</span><br><span class="line">        kernel_size = <span class="number">7</span></span><br><span class="line">        self.compress = ChannelPool()</span><br><span class="line">        self.spatial = BasicConv(<span class="number">2</span>, <span class="number">1</span>, kernel_size, stride=<span class="number">1</span>, padding=(kernel_size-<span class="number">1</span>) // <span class="number">2</span>, relu=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x_compress = self.compress(x)</span><br><span class="line">        x_out = self.spatial(x_compress)</span><br><span class="line">        scale = F.sigmoid(x_out) <span class="comment"># broadcasting</span></span><br><span class="line">        <span class="keyword">return</span> x * scale</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CBAM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, gate_channels, reduction_ratio=<span class="number">16</span>, pool_types=[<span class="string">'avg'</span>, <span class="string">'max'</span>], no_spatial=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CBAM, self).__init__()</span><br><span class="line">        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)</span><br><span class="line">        self.no_spatial=no_spatial</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> no_spatial:</span><br><span class="line">            self.SpatialGate = SpatialGate()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x_out = self.ChannelGate(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.no_spatial:</span><br><span class="line">            x_out = self.SpatialGate(x_out)</span><br><span class="line">        <span class="keyword">return</span> x_out</span><br></pre></td></tr></tbody></table></figure><p>æ”¾åœ¨æ¨¡å‹ä¸­çš„ä½ç½®<br><img src="https://img-blog.csdnimg.cn/c03140899eb54767bb2280596f9d2fd2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><h2 id="Coordinate-Attention"><a href="#Coordinate-Attention" class="headerlink" title="Coordinate Attention"></a>Coordinate Attention</h2><p>å‘è¡¨åœ¨CVPR2021<br>ç»“åˆä¸‹é¢ç»“æ„å›¾ï¼ŒCoordinate Attentionæ•´ä½“æ€è·¯æ˜¯ï¼Œå¯¹äºè¾“å…¥çš„ç‰¹å¾åˆ†åˆ«æŒ‰ç…§hæ–¹å‘å’Œwæ–¹å‘è¿›è¡Œæ± åŒ–ï¼Œä¹Ÿå°±æ˜¯å˜æˆcÃ—1Ã—wï¼ŒcÃ—hÃ—1ï¼Œ<br>ç„¶åå°†æ± åŒ–åçš„ç‰¹å¾è¿›è¡Œconcatæ‹¼æ¥ï¼Œæ³¨æ„ä¸æ˜¯ç›´æ¥æ‹¼æ¥ï¼Œå…ˆå°†ç»´åº¦è°ƒæ•´ä¸€æ ·ã€‚å› ä¸ºç»´åº¦ä¸ä¸€æ ·ï¼Œç›´æ¥æ‹¼æ¥ä¼šæœ‰å¹¿æ’­æœºåˆ¶ï¼Œ<br>åˆå¹¶åè¿›è¡Œ1Ã—1çš„å·ç§¯ç­‰ä¸€ç³»åˆ—æ“ä½œï¼Œæ­¤æ—¶å·ç§¯é€šé“æ•°å˜ä¸ºåŸæ¥çš„1/rï¼Œ<br>ç„¶åå†åˆ†å¼€ï¼Œåˆ†åˆ«åœ¨ä¸åŒçš„æ–¹å‘ä¸Šè¿›è¡Œsigmoidå¾—åˆ°ç³»æ•°ï¼Œç„¶åç›¸ä¹˜ã€‚</p><p>ğŸ˜‚æ•´ä¸ªç»“æ„å°±æ˜¯è¿™æ ·ï¼Œç¡®å®å¾ˆç„å­¦ï¼Œä½†æ˜¯æœ‰æ•ˆï¼Œè€Œä¸”æ•…äº‹è®²å¾—å¥½.<br><img src="https://img-blog.csdnimg.cn/d181bed69bad4bb7b1111dfdf1429e42.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"><br>è¯¥æ¨¡å—æ•´ä½“æ€è·¯å¦‚å›¾æ‰€ç¤º</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">h_sigmoid</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplace=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(h_sigmoid, self).__init__()</span><br><span class="line">        self.relu = nn.ReLU6(inplace=inplace)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.relu(x + <span class="number">3</span>) / <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">h_swish</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplace=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(h_swish, self).__init__()</span><br><span class="line">        self.sigmoid = h_sigmoid(inplace=inplace)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * self.sigmoid(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CoordAtt</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inp, oup, reduction=<span class="number">32</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CoordAtt, self).__init__()</span><br><span class="line">        self.pool_h = nn.AdaptiveAvgPool2d((<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool_w = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="literal">None</span>))</span><br><span class="line"></span><br><span class="line">        mip = <span class="built_in">max</span>(<span class="number">8</span>, inp // reduction)</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(inp, mip, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(mip)</span><br><span class="line">        self.act = h_swish()</span><br><span class="line">        </span><br><span class="line">        self.conv_h = nn.Conv2d(mip, oup, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.conv_w = nn.Conv2d(mip, oup, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        </span><br><span class="line">        n,c,h,w = x.size()</span><br><span class="line">        x_h = self.pool_h(x)</span><br><span class="line">        x_w = self.pool_w(x).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        y = torch.cat([x_h, x_w], dim=<span class="number">2</span>)</span><br><span class="line">        y = self.conv1(y)</span><br><span class="line">        y = self.bn1(y)</span><br><span class="line">        y = self.act(y) </span><br><span class="line">        </span><br><span class="line">        x_h, x_w = torch.split(y, [h, w], dim=<span class="number">2</span>)</span><br><span class="line">        x_w = x_w.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        a_h = self.conv_h(x_h).sigmoid()</span><br><span class="line">        a_w = self.conv_w(x_w).sigmoid()</span><br><span class="line"></span><br><span class="line">        out = identity * a_w * a_h</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><p>æ¥ä¸‹æ¥çœ‹ä¸€ä¸‹å…·ä½“çš„å¦‚ä½•ä½¿ç”¨ï¼šç›´æ¥åŠ å…¥åˆ°æ®‹å·®å—é‡Œé¢</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inp, oup, stride, expand_ratio</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> stride <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        hidden_dim = <span class="built_in">round</span>(inp * expand_ratio)</span><br><span class="line">        self.identity = stride == <span class="number">1</span> <span class="keyword">and</span> inp == oup</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> expand_ratio == <span class="number">1</span>:</span><br><span class="line">            self.conv = nn.Sequential(</span><br><span class="line">                <span class="comment"># dw</span></span><br><span class="line">                nn.Conv2d(hidden_dim, hidden_dim, <span class="number">3</span>, stride, <span class="number">1</span>, groups=hidden_dim, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden_dim),</span><br><span class="line">                nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">                <span class="comment"># pw-linear</span></span><br><span class="line">                nn.Conv2d(hidden_dim, oup, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(oup),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv = nn.Sequential(</span><br><span class="line">                <span class="comment"># pw</span></span><br><span class="line">                nn.Conv2d(inp, hidden_dim, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden_dim),</span><br><span class="line">                nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">                <span class="comment"># dw</span></span><br><span class="line">                nn.Conv2d(hidden_dim, hidden_dim, <span class="number">3</span>, stride, <span class="number">1</span>, groups=hidden_dim, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden_dim),</span><br><span class="line">                nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">                <span class="comment"># coordinate attention</span></span><br><span class="line">                CoordAtt(hidden_dim, hidden_dim),</span><br><span class="line">                <span class="comment"># pw-linear</span></span><br><span class="line">                nn.Conv2d(hidden_dim, oup, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(oup),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.conv(x)</span><br><span class="line">        <span class="keyword">if</span> self.identity:</span><br><span class="line">            <span class="keyword">return</span> x + y</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> y</span><br></pre></td></tr></tbody></table></figure><h2 id="ECA"><a href="#ECA" class="headerlink" title="ECA"></a>ECA</h2><p>ä»£ç æ¥è‡ªä»“åº“ï¼š<a href="https://github.com/BangguWu/ECANet[%E6%B7%BB%E5%8A%A0%E9%93%BE%E6%8E%A5%E6%8F%8F%E8%BF%B0](https://github.com/BangguWu/ECANet)">https://github.com/BangguWu/ECANet[æ·»åŠ é“¾æ¥æè¿°](https://github.com/BangguWu/ECANet)</a></p><p><img src="https://img-blog.csdnimg.cn/e6b364fac8cc464e8113d2edb179650f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°"></p><p>åŸºæœ¬æ€æƒ³ï¼šseæ¨¡å—æ˜¯ç»™æ¯ä¸€ä¸ªé€šé“ä¸€ä¸ªæƒé‡ï¼Œä¹Ÿå°±æ˜¯æ ¹æ®å½“å‰é€šé“çš„ç‰¹å¾ï¼Œç»™å‡ºä¸€ä¸ªæƒå€¼ï¼Œè€Œseä¸­å…¨è¿æ¥å±‚çš„é€šé“å´ç”±å¤§å˜å°å†å˜å¤§ï¼Œç‰¹å¾é€šé“å˜å°ä¹‹åï¼ŒåŸæ¥é€šé“æ•°å‘ç”Ÿè¾¹ï¼Œä¸å†å…·æœ‰æ¯ä¸ªé€šé“åŸæœ‰çš„ç‰¹å¾ã€‚å› æ­¤æå‡ºecaï¼ˆä¸çŸ¥é“ç†è§£çš„å¯¹ä¸å¯¹ï¼ï¼‰<br>é¦–å…ˆå°†è¾“å…¥çš„ç‰¹å¾é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–ï¼Œç„¶ååˆ©ç”¨1då·ç§¯è¿›è¡Œç‰¹å¾æå–ï¼Œå®ç°è·¨é€šé“çš„äº¤äº’ã€‚</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">eca_layer</span>(nn.Module):</span><br><span class="line">    <span class="string">"""Constructs a ECA module.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        channel: Number of channels of the input feature map</span></span><br><span class="line"><span class="string">        k_size: Adaptive selection of kernel size</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, k_size=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(eca_layer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv1d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=k_size, padding=(k_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>) </span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># feature descriptor on the global spatial information</span></span><br><span class="line">        y = self.avg_pool(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Two different branches of ECA module</span></span><br><span class="line">        y = self.conv(y.squeeze(-<span class="number">1</span>).transpose(-<span class="number">1</span>, -<span class="number">2</span>)).transpose(-<span class="number">1</span>, -<span class="number">2</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Multi-scale information fusion</span></span><br><span class="line">        y = self.sigmoid(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x * y.expand_as(x)</span><br><span class="line">        </span><br></pre></td></tr></tbody></table></figure><p>åç»­æœ‰æ—¶é—´æ…¢æ…¢è¡¥å……</p><h2 id="BAM"><a href="#BAM" class="headerlink" title="BAM"></a>BAM</h2><h2 id="RBFï¼ˆæ„Ÿå—é‡æ¨¡å—ï¼‰"><a href="#RBFï¼ˆæ„Ÿå—é‡æ¨¡å—ï¼‰" class="headerlink" title="RBFï¼ˆæ„Ÿå—é‡æ¨¡å—ï¼‰"></a>RBFï¼ˆæ„Ÿå—é‡æ¨¡å—ï¼‰</h2>]]></content>
      
      
      
        <tags>
            
            <tag> cv </tag>
            
            <tag> æ³¨æ„åŠ›æœºåˆ¶ </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
