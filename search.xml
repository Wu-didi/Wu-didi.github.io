<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>注意力SE CBAM CA ECA等模块整理</title>
      <link href="/2022/05/03/attention/"/>
      <url>/2022/05/03/attention/</url>
      
        <content type="html"><![CDATA[<h1 id="总结曾经使用过的一些即插即用的模块以及一些注意力机制"><a href="#总结曾经使用过的一些即插即用的模块以及一些注意力机制" class="headerlink" title="总结曾经使用过的一些即插即用的模块以及一些注意力机制"></a>总结曾经使用过的一些即插即用的模块以及一些注意力机制</h1><p>**</p><h2 id="注意力模块：SE"><a href="#注意力模块：SE" class="headerlink" title="注意力模块：SE"></a>注意力模块：SE</h2><p>**<br>代码源自这位大佬的仓库：<a href="https://github.com/moskomule/senet.pytorch">https://github.com/moskomule/senet.pytorch</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SELayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SELayer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(channel, channel // reduction, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(channel // reduction, channel, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.avg_pool(x).view(b, c)</span><br><span class="line">        y = self.fc(y).view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x * y.expand_as(x)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>SE模块理解起来比较简单，总体的思想是<strong>给每个特征图不同的权重</strong>，关注更有用的特征<br><img src="https://img-blog.csdnimg.cn/2fa44d6a35d04b9684ca36c0a8b65d08.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>具体做法<br>先对输入的特征图进行全局池化，将特征图变成1×1×通道数，然后全连接层和激活函数，对1×1×通道数的特征图进行调整，变成每一个特征图的权重，然后与输入的特征进行相乘。<br>缺点：没有考虑空间位置</p><p>SE模块的插入位置<br>通过Resnet的基础模块和bottleneck模块 可以看出SE模块插入到，跳连结构add之前，对前面特征提取之后的特征图给与不同的权重，再与shortcut跳连分支相加</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SEBasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 base_width=<span class="number">64</span>, dilation=<span class="number">1</span>, norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 *, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SEBasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes, <span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.se = SELayer(planes, reduction)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.se(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SEBottleneck</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 base_width=<span class="number">64</span>, dilation=<span class="number">1</span>, norm_layer=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 *, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SEBottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride,</span><br><span class="line">                               padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv3 = nn.Conv2d(planes, planes * <span class="number">4</span>, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(planes * <span class="number">4</span>)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.se = SELayer(planes * <span class="number">4</span>, reduction)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line">        out = self.se(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><h2 id="CBAM"><a href="#CBAM" class="headerlink" title="CBAM"></a>CBAM</h2><p>CBAM是解决se只考虑通道而忽略空间信息的弊端，提出的结构，通过下面的图很清晰的给出了该注意力模块的结构，先是类型与se的结构，产生不同的通道权重，也就是不同通道的重要程度。<br>然后将所有的特征图压缩到一个特征图，求空间特征的权重，很容易理解<br><img src="https://img-blog.csdnimg.cn/34c2738eb2ee4e45a181d40d766dfd59.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2272ca7990ee4ee893b348a6d09d799a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>代码来自:<a href="https://github.com/Jongchan/attention-module">https://github.com/Jongchan/attention-module</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment">#基础的卷积模块 由卷积层+BN+激活函数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_planes, out_planes, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, relu=<span class="literal">True</span>, bn=<span class="literal">True</span>, bias=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicConv, self).__init__()</span><br><span class="line">        self.out_channels = out_planes</span><br><span class="line">        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)</span><br><span class="line">        self.bn = nn.BatchNorm2d(out_planes,eps=<span class="number">1e-5</span>, momentum=<span class="number">0.01</span>, affine=<span class="literal">True</span>) <span class="keyword">if</span> bn <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        self.relu = nn.ReLU() <span class="keyword">if</span> relu <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">if</span> self.bn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.bn(x)</span><br><span class="line">        <span class="keyword">if</span> self.relu <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment">#展平层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Flatten</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line"><span class="comment">#通道注意</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelGate</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, gate_channels, reduction_ratio=<span class="number">16</span>, pool_types=[<span class="string">'avg'</span>, <span class="string">'max'</span>]</span>):</span><br><span class="line">        <span class="built_in">super</span>(ChannelGate, self).__init__()</span><br><span class="line">        self.gate_channels = gate_channels</span><br><span class="line">        self.mlp = nn.Sequential(</span><br><span class="line">            Flatten(),</span><br><span class="line">            nn.Linear(gate_channels, gate_channels // reduction_ratio),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(gate_channels // reduction_ratio, gate_channels)</span><br><span class="line">            )</span><br><span class="line">        self.pool_types = pool_types</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        channel_att_sum = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> pool_type <span class="keyword">in</span> self.pool_types:</span><br><span class="line">            <span class="keyword">if</span> pool_type==<span class="string">'avg'</span>:</span><br><span class="line">                avg_pool = F.avg_pool2d( x, (x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)), stride=(x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)))</span><br><span class="line">                channel_att_raw = self.mlp( avg_pool )</span><br><span class="line">            <span class="keyword">elif</span> pool_type==<span class="string">'max'</span>:</span><br><span class="line">                max_pool = F.max_pool2d( x, (x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)), stride=(x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)))</span><br><span class="line">                channel_att_raw = self.mlp( max_pool )</span><br><span class="line">            <span class="keyword">elif</span> pool_type==<span class="string">'lp'</span>:</span><br><span class="line">                lp_pool = F.lp_pool2d( x, <span class="number">2</span>, (x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)), stride=(x.size(<span class="number">2</span>), x.size(<span class="number">3</span>)))</span><br><span class="line">                channel_att_raw = self.mlp( lp_pool )</span><br><span class="line">            <span class="keyword">elif</span> pool_type==<span class="string">'lse'</span>:</span><br><span class="line">                <span class="comment"># LSE pool only</span></span><br><span class="line">                lse_pool = logsumexp_2d(x)</span><br><span class="line">                channel_att_raw = self.mlp( lse_pool )</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> channel_att_sum <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                channel_att_sum = channel_att_raw</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                channel_att_sum = channel_att_sum + channel_att_raw</span><br><span class="line"></span><br><span class="line">        scale = F.sigmoid( channel_att_sum ).unsqueeze(<span class="number">2</span>).unsqueeze(<span class="number">3</span>).expand_as(x)</span><br><span class="line">        <span class="keyword">return</span> x * scale</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logsumexp_2d</span>(<span class="params">tensor</span>):</span><br><span class="line">    tensor_flatten = tensor.view(tensor.size(<span class="number">0</span>), tensor.size(<span class="number">1</span>), -<span class="number">1</span>)</span><br><span class="line">    s, _ = torch.<span class="built_in">max</span>(tensor_flatten, dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    outputs = s + (tensor_flatten - s).exp().<span class="built_in">sum</span>(dim=<span class="number">2</span>, keepdim=<span class="literal">True</span>).log()</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelPool</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> torch.cat( (torch.<span class="built_in">max</span>(x,<span class="number">1</span>)[<span class="number">0</span>].unsqueeze(<span class="number">1</span>), torch.mean(x,<span class="number">1</span>).unsqueeze(<span class="number">1</span>)), dim=<span class="number">1</span> )</span><br><span class="line"><span class="comment">#空间注意力部分</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialGate</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SpatialGate, self).__init__()</span><br><span class="line">        kernel_size = <span class="number">7</span></span><br><span class="line">        self.compress = ChannelPool()</span><br><span class="line">        self.spatial = BasicConv(<span class="number">2</span>, <span class="number">1</span>, kernel_size, stride=<span class="number">1</span>, padding=(kernel_size-<span class="number">1</span>) // <span class="number">2</span>, relu=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x_compress = self.compress(x)</span><br><span class="line">        x_out = self.spatial(x_compress)</span><br><span class="line">        scale = F.sigmoid(x_out) <span class="comment"># broadcasting</span></span><br><span class="line">        <span class="keyword">return</span> x * scale</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CBAM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, gate_channels, reduction_ratio=<span class="number">16</span>, pool_types=[<span class="string">'avg'</span>, <span class="string">'max'</span>], no_spatial=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CBAM, self).__init__()</span><br><span class="line">        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)</span><br><span class="line">        self.no_spatial=no_spatial</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> no_spatial:</span><br><span class="line">            self.SpatialGate = SpatialGate()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x_out = self.ChannelGate(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.no_spatial:</span><br><span class="line">            x_out = self.SpatialGate(x_out)</span><br><span class="line">        <span class="keyword">return</span> x_out</span><br></pre></td></tr></tbody></table></figure><p>放在模型中的位置<br><img src="https://img-blog.csdnimg.cn/c03140899eb54767bb2280596f9d2fd2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="Coordinate-Attention"><a href="#Coordinate-Attention" class="headerlink" title="Coordinate Attention"></a>Coordinate Attention</h2><p>发表在CVPR2021<br>结合下面结构图，Coordinate Attention整体思路是，对于输入的特征分别按照h方向和w方向进行池化，也就是变成c×1×w，c×h×1，<br>然后将池化后的特征进行concat拼接，注意不是直接拼接，先将维度调整一样。因为维度不一样，直接拼接会有广播机制，<br>合并后进行1×1的卷积等一系列操作，此时卷积通道数变为原来的1/r，<br>然后再分开，分别在不同的方向上进行sigmoid得到系数，然后相乘。</p><p>😂整个结构就是这样，确实很玄学，但是有效，而且故事讲得好.<br><img src="https://img-blog.csdnimg.cn/d181bed69bad4bb7b1111dfdf1429e42.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>该模块整体思路如图所示</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">h_sigmoid</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplace=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(h_sigmoid, self).__init__()</span><br><span class="line">        self.relu = nn.ReLU6(inplace=inplace)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.relu(x + <span class="number">3</span>) / <span class="number">6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">h_swish</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplace=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(h_swish, self).__init__()</span><br><span class="line">        self.sigmoid = h_sigmoid(inplace=inplace)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x * self.sigmoid(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CoordAtt</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inp, oup, reduction=<span class="number">32</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CoordAtt, self).__init__()</span><br><span class="line">        self.pool_h = nn.AdaptiveAvgPool2d((<span class="literal">None</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool_w = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="literal">None</span>))</span><br><span class="line"></span><br><span class="line">        mip = <span class="built_in">max</span>(<span class="number">8</span>, inp // reduction)</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(inp, mip, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(mip)</span><br><span class="line">        self.act = h_swish()</span><br><span class="line">        </span><br><span class="line">        self.conv_h = nn.Conv2d(mip, oup, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.conv_w = nn.Conv2d(mip, oup, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        </span><br><span class="line">        n,c,h,w = x.size()</span><br><span class="line">        x_h = self.pool_h(x)</span><br><span class="line">        x_w = self.pool_w(x).permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        y = torch.cat([x_h, x_w], dim=<span class="number">2</span>)</span><br><span class="line">        y = self.conv1(y)</span><br><span class="line">        y = self.bn1(y)</span><br><span class="line">        y = self.act(y) </span><br><span class="line">        </span><br><span class="line">        x_h, x_w = torch.split(y, [h, w], dim=<span class="number">2</span>)</span><br><span class="line">        x_w = x_w.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        a_h = self.conv_h(x_h).sigmoid()</span><br><span class="line">        a_w = self.conv_w(x_w).sigmoid()</span><br><span class="line"></span><br><span class="line">        out = identity * a_w * a_h</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><p>接下来看一下具体的如何使用：直接加入到残差块里面</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inp, oup, stride, expand_ratio</span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> stride <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        hidden_dim = <span class="built_in">round</span>(inp * expand_ratio)</span><br><span class="line">        self.identity = stride == <span class="number">1</span> <span class="keyword">and</span> inp == oup</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> expand_ratio == <span class="number">1</span>:</span><br><span class="line">            self.conv = nn.Sequential(</span><br><span class="line">                <span class="comment"># dw</span></span><br><span class="line">                nn.Conv2d(hidden_dim, hidden_dim, <span class="number">3</span>, stride, <span class="number">1</span>, groups=hidden_dim, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden_dim),</span><br><span class="line">                nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">                <span class="comment"># pw-linear</span></span><br><span class="line">                nn.Conv2d(hidden_dim, oup, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(oup),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv = nn.Sequential(</span><br><span class="line">                <span class="comment"># pw</span></span><br><span class="line">                nn.Conv2d(inp, hidden_dim, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden_dim),</span><br><span class="line">                nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">                <span class="comment"># dw</span></span><br><span class="line">                nn.Conv2d(hidden_dim, hidden_dim, <span class="number">3</span>, stride, <span class="number">1</span>, groups=hidden_dim, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(hidden_dim),</span><br><span class="line">                nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">                <span class="comment"># coordinate attention</span></span><br><span class="line">                CoordAtt(hidden_dim, hidden_dim),</span><br><span class="line">                <span class="comment"># pw-linear</span></span><br><span class="line">                nn.Conv2d(hidden_dim, oup, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(oup),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.conv(x)</span><br><span class="line">        <span class="keyword">if</span> self.identity:</span><br><span class="line">            <span class="keyword">return</span> x + y</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> y</span><br></pre></td></tr></tbody></table></figure><h2 id="ECA"><a href="#ECA" class="headerlink" title="ECA"></a>ECA</h2><p>代码来自仓库：<a href="https://github.com/BangguWu/ECANet[%E6%B7%BB%E5%8A%A0%E9%93%BE%E6%8E%A5%E6%8F%8F%E8%BF%B0](https://github.com/BangguWu/ECANet)">https://github.com/BangguWu/ECANet[添加链接描述](https://github.com/BangguWu/ECANet)</a></p><p><img src="https://img-blog.csdnimg.cn/e6b364fac8cc464e8113d2edb179650f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>基本思想：se模块是给每一个通道一个权重，也就是根据当前通道的特征，给出一个权值，而se中全连接层的通道却由大变小再变大，特征通道变小之后，原来通道数发生边，不再具有每个通道原有的特征。因此提出eca（不知道理解的对不对！）<br>首先将输入的特征通过全局平均池化，然后利用1d卷积进行特征提取，实现跨通道的交互。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">eca_layer</span>(nn.Module):</span><br><span class="line">    <span class="string">"""Constructs a ECA module.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        channel: Number of channels of the input feature map</span></span><br><span class="line"><span class="string">        k_size: Adaptive selection of kernel size</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, k_size=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(eca_layer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.conv = nn.Conv1d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=k_size, padding=(k_size - <span class="number">1</span>) // <span class="number">2</span>, bias=<span class="literal">False</span>) </span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># feature descriptor on the global spatial information</span></span><br><span class="line">        y = self.avg_pool(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Two different branches of ECA module</span></span><br><span class="line">        y = self.conv(y.squeeze(-<span class="number">1</span>).transpose(-<span class="number">1</span>, -<span class="number">2</span>)).transpose(-<span class="number">1</span>, -<span class="number">2</span>).unsqueeze(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Multi-scale information fusion</span></span><br><span class="line">        y = self.sigmoid(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x * y.expand_as(x)</span><br><span class="line">        </span><br></pre></td></tr></tbody></table></figure><p>后续有时间慢慢补充</p><h2 id="BAM"><a href="#BAM" class="headerlink" title="BAM"></a>BAM</h2><h2 id="RBF（感受野模块）"><a href="#RBF（感受野模块）" class="headerlink" title="RBF（感受野模块）"></a>RBF（感受野模块）</h2>]]></content>
      
      
      
        <tags>
            
            <tag> cv </tag>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
