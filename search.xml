<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习、深度学习笔试题 面试题总结</title>
      <link href="/2022/05/11/mianshi/"/>
      <url>/2022/05/11/mianshi/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习、深度学习笔试题-面试题总结"><a href="#机器学习、深度学习笔试题-面试题总结" class="headerlink" title="机器学习、深度学习笔试题 面试题总结"></a>机器学习、深度学习笔试题 面试题总结</h1><p>一切都是为了找工作！！！<br>整理看到的内容，以免忘记<br>（持续更新）</p><h2 id="感受野如何计算？"><a href="#感受野如何计算？" class="headerlink" title="感受野如何计算？"></a>感受野如何计算？</h2><p>参考链接：<a href="https://blog.csdn.net/a841454735/article/details/88558906">https://blog.csdn.net/a841454735/article/details/88558906</a><br>感受野指的是一个特定的 CNN 特征（特征图上的某个点）在输入空间所受影响的区域。<br>就是特征图上的一点对应输入特征图的区域</p><p>a)第一层卷积层的输出特征图像素的感受野的大小等于滤波器的大小；</p><p>b)深层卷积层的感受野大小和它之前所有层的滤波器大小和步长有关系；</p><p>c)计算感受野大小时，忽略了图像边缘的影响，即不考虑padding的大小。</p><p>计算公式<br>**方法1 **<br>从原始input出发，逐层迭代到最后输出：<br><img src="https://img-blog.csdnimg.cn/69963e1a81b3412291effbb7bcc51902.png" alt="在这里插入图片描述"><br>其中 lk-1 为第 k-1 层对应的感受野大小，fk 为第k 层的卷积核大小，或者是池化层的池化尺寸大小。si为步长</p><p><strong>top to down的方式</strong><br>即从网络的最后向前推<br>感受野的大小是由kenel size和stride size 一起决定的</p><p>rfsize = f(out, stride, ksize) = (out - 1) * stride + ksize，<br>其中out是指上一层感受野的大小，stride是当前层stride<br>最后一层不带入公式，它的ksize是前一层的out</p><h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><p>参考链接：<a href="https://blog.csdn.net/weixin_44211398/article/details/91613803?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0.pc_relevant_paycolumn_v3&amp;spm=1001.2101.3001.4242.1&amp;utm_relevant_index=3">RPN详解</a><br>参考链接2：<a href="https://zhuanlan.zhihu.com/p/31426458">一文读懂Faster RCNN</a><br>RPN全称是Region Proposal Network，Region Proposal的中文意思是“区域选取”，也就是“提取候选框”的意思，所以RPN就是用来提取候选框的网络</p><p>RPN第一次出现在世人眼中是在Faster RCNN这个结构中，专门用来提取候选框，在RCNN和Fast RCNN等物体检测架构中，用来提取候选框的方法通常是Selective Search，是比较传统的方法，而且比较耗时，在CPU上要2s一张图。所以作者提出RPN，专门用来提取候选框，一方面RPN耗时少，另一方面RPN可以很容易结合到Fast RCNN中，称为一个整体。</p><p><img src="https://img-blog.csdnimg.cn/b44f863c292740d9a16978ade6bce3e1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p><strong>整体流程：</strong><br>最后我们再把RPN整个流程走一遍，首先通过一系列卷积得到公共特征图，假设他的大小是N x 16 x 16，然后我们进入RPN阶段，首先经过一个3 x 3的卷积，得到一个256 x 16 x 16的特征图，也可以看作16 x 16个256维特征向量，然后经过两次1 x 1的卷积，分别得到一个18 x 16 x 16的特征图，和一个36 x 16 x 16的特征图，也就是16 x 16 x 9个结果，每个结果包含2个分数和4个坐标，再结合预先定义的Anchors，经过后处理，就得到候选框</p><h2 id="AUC是什么？怎么计算"><a href="#AUC是什么？怎么计算" class="headerlink" title="AUC是什么？怎么计算"></a>AUC是什么？怎么计算</h2><p>AUC就是ROC曲线与X轴所围成的面积</p><h2 id="介绍可变形卷积"><a href="#介绍可变形卷积" class="headerlink" title="介绍可变形卷积"></a>介绍可变形卷积</h2><p><strong>参考链接：<a href="(https://blog.csdn.net/LEEANG121/article/details/104234927)">[可变形卷积从概念到实现过程]</a></strong><br>可变形卷积是指卷积核在每一个元素上额外增加了一个参数方向参数，这样卷积核就能在训练过程中扩展到很大的范围。<br>我们知道卷积核的目的是为了提取输入物的特征。我们传统的卷积核通常是固定尺寸、固定大小的（例如3x3，5x5，7x7.）。这种卷积核存在的最大问题就是，对于未知的变化适应性差，泛化能力不强。</p><p>卷积单元对输入的特征图在固定的位置进行采样；池化层不断减小着特征图的尺寸；RoI池化层产生空间位置受限的RoI。网络内部缺乏能够解决这个问题的模块，这会产生显著的问题，例如，同一CNN层的激活单元的感受野尺寸都相同，这对于编码位置信息的浅层神经网络并不可取，因为不同的位置可能对应有不同尺度或者不同形变的物体，这些层需要能够自动调整尺度或者感受野的方法。再比如，目标检测虽然效果很好但是都依赖于基于特征提取的边界框，这并不是最优的方法，尤其是对于非网格状的物体而言。</p><h2 id="介绍Transformer公式"><a href="#介绍Transformer公式" class="headerlink" title="介绍Transformer公式"></a>介绍Transformer公式</h2><p>参考链接：**<a href="https://www.zhihu.com/question/325839123/answer/1903376265">深度学习attention机制中的Q,K,V分别是从哪来的</a>**</p><h2 id="有哪些代表性的Attention？"><a href="#有哪些代表性的Attention？" class="headerlink" title="有哪些代表性的Attention？"></a>有哪些代表性的Attention？</h2><p>之前写过注意力的博文</p><h2 id="介绍常用的移动端-x2F-轻量化模型"><a href="#介绍常用的移动端-x2F-轻量化模型" class="headerlink" title="介绍常用的移动端/轻量化模型"></a>介绍常用的移动端/轻量化模型</h2><h2 id="XGboost是怎么做特征筛选的？"><a href="#XGboost是怎么做特征筛选的？" class="headerlink" title="XGboost是怎么做特征筛选的？"></a>XGboost是怎么做特征筛选的？</h2><h2 id="CNN中可以不用池化么？为什么？"><a href="#CNN中可以不用池化么？为什么？" class="headerlink" title="CNN中可以不用池化么？为什么？"></a>CNN中可以不用池化么？为什么？</h2><h2 id="介绍目标检测全部流程（输入到输出）"><a href="#介绍目标检测全部流程（输入到输出）" class="headerlink" title="介绍目标检测全部流程（输入到输出）"></a>介绍目标检测全部流程（输入到输出）</h2><h2 id="手推-LR"><a href="#手推-LR" class="headerlink" title="手推 LR"></a>手推 LR</h2><p>参考链接：<a href="https://blog.csdn.net/weixin_43249038/article/details/107315779">【here】</a></p><h2 id="介绍知识蒸馏原理"><a href="#介绍知识蒸馏原理" class="headerlink" title="介绍知识蒸馏原理"></a>介绍知识蒸馏原理</h2><h2 id="ReLU和ReLU6的区别"><a href="#ReLU和ReLU6的区别" class="headerlink" title="ReLU和ReLU6的区别"></a>ReLU和ReLU6的区别</h2><p>参考链接:<a href="https://blog.csdn.net/zisuina_2/article/details/113603346?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165197750816782395390550%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165197750816782395390550&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-113603346-null-null.142%5Ev9%5Epc_search_result_control_group,157%5Ev4%5Econtrol&amp;utm_term=relu6&amp;spm=1018.2226.3001.4187">【here】</a></p><h2 id="KNN-和-K-means介绍一下"><a href="#KNN-和-K-means介绍一下" class="headerlink" title="KNN 和 K-means介绍一下"></a>KNN 和 K-means介绍一下</h2><p>这个比较简单</p><h2 id="Bagging和Boosting的区别"><a href="#Bagging和Boosting的区别" class="headerlink" title="Bagging和Boosting的区别"></a>Bagging和Boosting的区别</h2><p><a href="https://blog.csdn.net/u012587024/article/details/86300168">参考链接1</a><br><a href="https://blog.csdn.net/u014114990/article/details/50948079?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromBaidu~default-1-50948079-blog-86300168.pc_relevant_without_ctrlist_v4&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromBaidu~default-1-50948079-blog-86300168.pc_relevant_without_ctrlist_v4&amp;utm_relevant_index=1">参考链接2</a></p><p><strong>bagging</strong>：bootstrap aggregating的缩写。让该学习算法训练多轮，每轮的训练集由从初始的训练集中随机取出的n个训练样本组成，某个初始训练样本在某轮训练集中可以出现多次或根本不出现，训练之后可得到一个预测函数序列h_1，⋯ ⋯h_n ，最终的预测函数H对分类问题采用投票方式，对回归问题采用简单平均方法对新示例进行判别。</p><p>训练R个分类器f_i，分类器之间其他相同就是参数不同。其中f_i是通过从训练集合中(N篇文档)随机取(取后放回)N次文档构成的训练集合训练得到的。对于新文档d，用这R个分类器去分类，得到的最多的那个类别作为d的最终类别。</p><p><strong>boosting</strong>: 其中主要的是AdaBoost（Adaptive Boosting）。初始化时对每一个训练例赋相等的权重1／n，然后用该学算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在后续的学习中集中对比较难的训练例进行学习，从而得到一个预测函数序列h_1,⋯, h_m , 其中h_i也有一定的权重，预测效果好的预测函数权重较大，反之较小。最终的预测函数H对分类问题采用有权重的投票方式，对回归问题采用加权平均的方法对新示例进行判别。<br>（类似Bagging方法，但是训练是串行进行的，第k个分类器训练时关注对前k-1分类器中错分的文档，即不是随机取，而是加大取这些文档的概率。)<br> （pku， sewm，shinningmonster.）</p><p><strong>Bagging与Boosting的区别</strong>：二者的主要区别是取样方式不同。Bagging采用均匀取样，而Boosting根据错误率来取样，因此Boosting的分类精度要优于Bagging。Bagging的训练集的选择是随机的，各轮训练集之间相互独立，而Boostlng的各轮训练集的选择与前面各轮的学习结果有关；Bagging的各个预测函数没有权重，而Boosting是有权重的；Bagging的各个预测函数可以并行生成，而Boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。Bagging可通过并行训练节省大量时间开销。<br>bagging和boosting都可以有效地提高分类的准确性。在大多数数据集中，boosting的准确性比bagging高。在有些数据集中，boosting会引起退化— Overfit。<br>Boosting思想的一种改进型 AdaBoost方法在邮件过滤、文本分类方面都有很好的性能。</p><p>**gradient boosting（又叫Mart, Treenet)**：Boosting是一种思想，Gradient Boosting是一种实现Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错。如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度（Gradient)的方向上下降。</p><p><strong>Rand forest</strong>： 随机森林，顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 在建立每一棵决策树的过程中，有两点需要注意 - 采样与完全分裂。首先是两个随机采样的过程，random forest对输入的数据要进行行、列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m &lt;&lt; M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。一般很多的决策树算法都一个重要的步骤 - 剪枝，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。 按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。<br><em>可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个feature中选择m让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。</em></p><p>Rand forest与bagging的区别：1）. Rand forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比输入样本的数目少的样本；2）. bagging是用全部特征来得到分类器，而rand forest是需要从全部特征中选取其中的一部分来训练得到分类器； 一般Rand forest效果比bagging效果好！</p><h2 id="小卷积核和大卷积核的应用场景"><a href="#小卷积核和大卷积核的应用场景" class="headerlink" title="小卷积核和大卷积核的应用场景"></a>小卷积核和大卷积核的应用场景</h2><h2 id="怎么处理长尾问题？从样本，模型的角度来看"><a href="#怎么处理长尾问题？从样本，模型的角度来看" class="headerlink" title="怎么处理长尾问题？从样本，模型的角度来看"></a>怎么处理长尾问题？从样本，模型的角度来看</h2><p>以下是个人的回答：<br>样本方面：通过对样本量较少的类别进行过采样，利用一些图像增强的方法，生成图像，达到各类别均衡。缺点是容易过拟合<br>模型角度：1 对损失函数进行改进，给予小样本的更多权值。  2 针对梯度进行改进，（还不太熟悉）</p><h2 id="机器学习笔试题"><a href="#机器学习笔试题" class="headerlink" title="机器学习笔试题"></a>机器学习笔试题</h2><h3 id="bootstrap统计抽样方法：有放回地从总共N个样本中抽样n个样本。"><a href="#bootstrap统计抽样方法：有放回地从总共N个样本中抽样n个样本。" class="headerlink" title="bootstrap统计抽样方法：有放回地从总共N个样本中抽样n个样本。"></a>bootstrap统计抽样方法：有放回地从总共N个样本中抽样n个样本。</h3><p>基于bootstrap，有以下常用的机器学习方法</p><ul><li>boosting</li><li>bagging</li><li>random forest（RF, 随机森林）<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3>基于规则的排序方案 :依据规则质量的某种度量对规则进行排序。这种排序方案确保每一个测试记录都是由覆盖它的”最好的”规则来分类。<br>基于类的排序方案:属于同一个类的规则在规则集R中一起出现。然后，这些规则根据它们所属的类信息一起排序</li></ul><h3 id="机器学习中L1正则化和L2正则化的区别是？"><a href="#机器学习中L1正则化和L2正则化的区别是？" class="headerlink" title="机器学习中L1正则化和L2正则化的区别是？"></a>机器学习中L1正则化和L2正则化的区别是？</h3><p>答案：<br>使用L1可以得到稀疏的权值<br>使用L2可以得到平滑的权值<br>一些解析：</p><p><img src="https://img-blog.csdnimg.cn/07b6529fff7249bc8deb2054dc0f2013.png" alt="在这里插入图片描述"><br>参考链接:<a href="https://blog.csdn.net/zouxy09/article/details/24971995/">【机器学习中的范数规则化之（一）L0、L1与L2范数】</a></p><h3 id="机器学习中做特征选择时，可能用到的方法有？"><a href="#机器学习中做特征选择时，可能用到的方法有？" class="headerlink" title="机器学习中做特征选择时，可能用到的方法有？"></a>机器学习中做特征选择时，可能用到的方法有？</h3><p>卡方 信息增益 平均互信息 期望交叉熵</p><p>在文本分类中，首先要对数据进行特征提取，特征提取中又分为特征选择和特征抽取两大类，在特征选择算法中有互信息，文档频率，信息增益，卡方检验以及期望交叉熵。<br>期望交叉熵，以文本分类为例子，期望交叉熵用来度量一个词对于整体的重要程度。<br>在ID3决策树中，也使用信息增益作为特征选择的方法，在C4.5决策树中，使用信息增益比作为特征选择的方法，在CART中，使用基尼指数作为特征选择的方法</p><h3 id="以下哪些方法不可以直接来对文本分类？"><a href="#以下哪些方法不可以直接来对文本分类？" class="headerlink" title="以下哪些方法不可以直接来对文本分类？"></a>以下哪些方法不可以直接来对文本分类？</h3><p>Kmeans 决策树 支持向量机 KNN<br>答案是Kmeans<br>不太懂<br>kmeans和knn之间的区别<br>参考链接：<a href="https://blog.csdn.net/cranberrycookie/article/details/91880674?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165223971216781432910782%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=165223971216781432910782&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-1-91880674-null-null.142%5Ev9%5Epc_search_result_control_group,157%5Ev4%5Econtrol&amp;utm_term=kmeans%E5%92%8Cknn%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB&amp;spm=1018.2226.3001.4187">kmeans和knn相同点和不同点</a></p><p>区别1：聚类和分类最大的不同在于，分类的目标是事先已知的，而聚类则不一样，聚类事先不知道目标变量是什么，类别没有像分类那样被预先定义出来，所以，聚类有时也叫无监督学习。聚类分析试图将相似的对象归入同一簇，将不相似的对象归为不同簇</p><p>区别2：K-means算法虽然比较容易实现，但是其可能收敛到局部最优解，且在大规模数据集上收敛速度相对较慢。</p><h3 id="下列哪个不属于CRF模型对于HMM和MEMM模型的优势（-）"><a href="#下列哪个不属于CRF模型对于HMM和MEMM模型的优势（-）" class="headerlink" title="下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）"></a>下列哪个不属于CRF模型对于HMM和MEMM模型的优势（ ）</h3><p>特征灵活<br>速度快  ✔<br>可容纳较多上下文信息<br>全局最优<br>——解析<br><a href="https://blog.csdn.net/weixin_40103562/article/details/109780449?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165224011816782391838789%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165224011816782391838789&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-109780449-null-null.142%5Ev9%5Epc_search_result_control_group,157%5Ev4%5Econtrol&amp;utm_term=CRF&amp;spm=1018.2226.3001.4187">CRF模型详解</a></p><h3 id="下列方法中，可以用于特征降维的方法包括（）"><a href="#下列方法中，可以用于特征降维的方法包括（）" class="headerlink" title="下列方法中，可以用于特征降维的方法包括（）"></a>下列方法中，可以用于特征降维的方法包括（）</h3><p>主成分分析PCA  ✔<br>线性判别分析LDA  ✔<br>深度学习SparseAutoEncoder  ✔<br>矩阵奇异值分解SVD  ✔<br>最小二乘法LeastSquares</p><h3 id="Nave-Bayes是一种特殊的Bayes分类器-特征变量是X-类别标签是C-它的一个假定是"><a href="#Nave-Bayes是一种特殊的Bayes分类器-特征变量是X-类别标签是C-它的一个假定是" class="headerlink" title="Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是:()"></a>Nave Bayes是一种特殊的Bayes分类器,特征变量是X,类别标签是C,它的一个假定是:()</h3><p>正确答案: C<br>各类别的先验概率P(C)是相等的<br>以0为均值，sqr(2)/2为标准差的正态分布<br>特征变量X的各个维度是类别条件独立随机变量<br>P(X|C)是高斯分布</p><h3 id="以下哪个模型是生成式模型："><a href="#以下哪个模型是生成式模型：" class="headerlink" title="以下哪个模型是生成式模型："></a>以下哪个模型是生成式模型：</h3><p>正确答案: A<br>贝叶斯模型<br>逻辑回归<br>SVM<br>条件随机场</p><p>————<br>生成模型，就是生成(数据的分布)的模型；<br>判别模型，就是判别(数据输出量)的模型。<br><strong>生成式模型：</strong></p><ul><li>朴素贝叶斯</li><li>混合高斯模型</li><li>隐马尔科夫模型(HMM)</li><li>贝叶斯网络</li><li>Sigmoid Belief Networks</li><li>马尔科夫随机场(Markov Random Fields)</li><li>深度信念网络(DBN)  </li><li><strong>判别式模型：</strong></li><li>K近邻(KNN)</li><li>线性回归(Linear Regression)</li><li>逻辑斯蒂回归(Logistic Regression)</li><li>神经网络(NN)</li><li>支持向量机(SVM)</li><li>高斯过程(Gaussian Process)</li><li>条件随机场(CRF)</li><li>CART(Classification and Regression Tree)</li></ul><h3 id="以下关于正则化的描述正确的是（）"><a href="#以下关于正则化的描述正确的是（）" class="headerlink" title="以下关于正则化的描述正确的是（）"></a>以下关于正则化的描述正确的是（）</h3><p>正确答案: A B C D<br>正则化可以防止过拟合<br>L1正则化能得到稀疏解<br>L2正则化约束了解空间<br>Dropout也是一种正则化方法</p><h3 id="在统计模式识分类问题中，当先验概率未知时，可以使用（）"><a href="#在统计模式识分类问题中，当先验概率未知时，可以使用（）" class="headerlink" title="在统计模式识分类问题中，当先验概率未知时，可以使用（）"></a>在统计模式识分类问题中，当先验概率未知时，可以使用（）</h3><p>正确答案: B C<br>最小损失准则<br>N-P判决<br>最小最大损失准则<br>最小误判概率准则</p><h3 id="以下-属于线性分类器最佳准则"><a href="#以下-属于线性分类器最佳准则" class="headerlink" title="以下()属于线性分类器最佳准则?"></a>以下()属于线性分类器最佳准则?</h3><p>正确答案: A C D<br>感知准则函数<br>贝叶斯分类<br>支持向量机<br>Fisher准则<br>——————<br>线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。<br>感知器准则函数：代价函数J=-(W*X+w0)，分类的准则是最小化代价函数。感知器是神经网络（NN）的基础，网上有很多介绍。<br>SVM：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题）<br>Fisher准则：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。<br>贝叶斯分类器：一种基于统计方法的分类器，要求先了解样本的分布特点（高斯、指数等），所以使用起来限制很多。在满足一些特定条件下，其优化目标与线性分类器有相同结构（同方差高斯分布等），其余条件下不是线性分类。</p><h3 id="下列哪些方法可以用来对高维数据进行降维"><a href="#下列哪些方法可以用来对高维数据进行降维" class="headerlink" title="下列哪些方法可以用来对高维数据进行降维:"></a>下列哪些方法可以用来对高维数据进行降维:</h3><p>正确答案: A B C D E F<br>LASSO<br>主成分分析法<br>聚类分析<br>小波分析法<br>线性判别法<br>拉普拉斯特征映射</p><h2 id="😊深度学习笔试题"><a href="#😊深度学习笔试题" class="headerlink" title="😊深度学习笔试题"></a>😊深度学习笔试题</h2><h3 id="ResNet-50-有多少个卷积层"><a href="#ResNet-50-有多少个卷积层" class="headerlink" title="ResNet-50 有多少个卷积层?"></a>ResNet-50 有多少个卷积层?</h3><p>49个<br>49个卷积层+1个全连接层</p><h3 id="Dropout不影响测试时间"><a href="#Dropout不影响测试时间" class="headerlink" title="Dropout不影响测试时间"></a>Dropout不影响测试时间</h3><h3 id="下列哪些项所描述的相关技术是错误的？"><a href="#下列哪些项所描述的相关技术是错误的？" class="headerlink" title="下列哪些项所描述的相关技术是错误的？"></a>下列哪些项所描述的相关技术是错误的？</h3><p>正确答案: C<br>AdaGrad使用的是一阶差分(first order differentiation)<br>L-BFGS使用的是二阶差分(second order differentiation)<br>AdaGrad使用的是二阶差分<br>————<br>AdaGrad是梯度下降法，用的是一阶导数信息，L-BFGS是拟牛顿法，在牛顿法的基础上发展而来的，用到了二阶导数信息。<br>牛顿法、拟牛顿法、adam，rmsgrad用到了二阶导<br>momentum、adgrad用的是一阶导，adgrad的特点是历史梯度正则。</p><h3 id="深度学习中的激活函数需要具有哪些属性？（）"><a href="#深度学习中的激活函数需要具有哪些属性？（）" class="headerlink" title="深度学习中的激活函数需要具有哪些属性？（）"></a>深度学习中的激活函数需要具有哪些属性？（）</h3><p>正确答案: A B D<br>计算简单<br>非线性<br>具有饱和区<br>几乎处处可微<br>————<br>参考链接：<a href="https://blog.csdn.net/zhanghao3389/article/details/85267461">【激活函数必要的属性】</a></p><ol><li>非线性：<br>即导数不是常数。这个条件是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。</li><li>几乎处处可微：<br>可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响。</li><li>计算简单：<br>非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。</li><li>非饱和性（saturation）：<br>饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x&gt;0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x&lt;0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU和PReLU的提出正是为了解决这一问题。</li><li>单调性（monotonic）：<br>即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。</li><li>输出范围有限：<br>有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响）、LSTM里的gate函数。</li><li>接近恒等变换（identity）：<br>即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x&gt;0时为线性。这个性质也让初始化参数范围的推导更为简单。这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet和RNN中的LSTM。</li><li>参数少：<br>大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。<br>归一化（normalization）：<br>这个是最近才出来的概念，对应的激活函数是SELU，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization。</li></ol><p><img src="https://img-blog.csdnimg.cn/86ba723a5c29401baca21569a5a759b7.png" alt="在这里插入图片描述"></p>]]></content>
      
      
      
        <tags>
            
            <tag> 注意力机制 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>注意力SE CBAM CA ECA等模块整理</title>
      <link href="/2022/05/03/attention/"/>
      <url>/2022/05/03/attention/</url>
      
        <content type="html"><![CDATA[<h1 id="总结曾经使用过的一些即插即用的模块以及一些注意力机制"><a href="#总结曾经使用过的一些即插即用的模块以及一些注意力机制" class="headerlink" title="总结曾经使用过的一些即插即用的模块以及一些注意力机制"></a>总结曾经使用过的一些即插即用的模块以及一些注意力机制</h1><h2 id="注意力模块：SE"><a href="#注意力模块：SE" class="headerlink" title="注意力模块：SE"></a>注意力模块：SE</h2><p>代码源自这位大佬的仓库：<a href="https://github.com/moskomule/senet.pytorch">https://github.com/moskomule/senet.pytorch</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SELayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>SELayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>avg_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channel<span class="token punctuation">,</span> channel <span class="token operator">//</span> reduction<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>channel <span class="token operator">//</span> reduction<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> x <span class="token operator">*</span> y<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span></code></pre><p>SE模块理解起来比较简单，总体的思想是<strong>给每个特征图不同的权重</strong>，关注更有用的特征<br><img src="https://img-blog.csdnimg.cn/2fa44d6a35d04b9684ca36c0a8b65d08.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>具体做法<br>先对输入的特征图进行全局池化，将特征图变成1×1×通道数，然后全连接层和激活函数，对1×1×通道数的特征图进行调整，变成每一个特征图的权重，然后与输入的特征进行相乘。<br>缺点：没有考虑空间位置</p><p>SE模块的插入位置<br>通过Resnet的基础模块和bottleneck模块 可以看出SE模块插入到，跳连结构add之前，对前面特征提取之后的特征图给与不同的权重，再与shortcut跳连分支相加</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SEBasicBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    expansion <span class="token operator">=</span> <span class="token number">1</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inplanes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> downsample<span class="token operator">=</span>None<span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                 base_width<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>None<span class="token punctuation">,</span>                 <span class="token operator">*</span><span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>SEBasicBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> conv3x3<span class="token punctuation">(</span>inplanes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> stride<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>planes<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> conv3x3<span class="token punctuation">(</span>planes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>planes<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>se <span class="token operator">=</span> SELayer<span class="token punctuation">(</span>planes<span class="token punctuation">,</span> reduction<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> downsample        self<span class="token punctuation">.</span>stride <span class="token operator">=</span> stride    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        residual <span class="token operator">=</span> x        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>se<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            residual <span class="token operator">=</span> self<span class="token punctuation">.</span>downsample<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out <span class="token operator">+=</span> residual        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        <span class="token keyword">return</span> out<span class="token keyword">class</span> <span class="token class-name">SEBottleneck</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    expansion <span class="token operator">=</span> <span class="token number">4</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inplanes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> downsample<span class="token operator">=</span>None<span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                 base_width<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> norm_layer<span class="token operator">=</span>None<span class="token punctuation">,</span>                 <span class="token operator">*</span><span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>SEBottleneck<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>inplanes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>planes<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>planes<span class="token punctuation">,</span> planes<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span>                               padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>planes<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>planes<span class="token punctuation">,</span> planes <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>planes <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>se <span class="token operator">=</span> SELayer<span class="token punctuation">(</span>planes <span class="token operator">*</span> <span class="token number">4</span><span class="token punctuation">,</span> reduction<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>downsample <span class="token operator">=</span> downsample        self<span class="token punctuation">.</span>stride <span class="token operator">=</span> stride    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        residual <span class="token operator">=</span> x        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>bn3<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        out <span class="token operator">=</span> self<span class="token punctuation">.</span>se<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>downsample <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            residual <span class="token operator">=</span> self<span class="token punctuation">.</span>downsample<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        out <span class="token operator">+=</span> residual        out <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>out<span class="token punctuation">)</span>        <span class="token keyword">return</span> out</code></pre><h2 id="CBAM"><a href="#CBAM" class="headerlink" title="CBAM"></a>CBAM</h2><p>CBAM是解决se只考虑通道而忽略空间信息的弊端，提出的结构，通过下面的图很清晰的给出了该注意力模块的结构，先是类型与se的结构，产生不同的通道权重，也就是不同通道的重要程度。<br>然后将所有的特征图压缩到一个特征图，求空间特征的权重，很容易理解<br><img src="https://img-blog.csdnimg.cn/34c2738eb2ee4e45a181d40d766dfd59.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2272ca7990ee4ee893b348a6d09d799a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>代码来自:<a href="https://github.com/Jongchan/attention-module">https://github.com/Jongchan/attention-module</a></p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> math<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token comment" spellcheck="true">#基础的卷积模块 由卷积层+BN+激活函数</span><span class="token keyword">class</span> <span class="token class-name">BasicConv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_planes<span class="token punctuation">,</span> out_planes<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> bn<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>BasicConv<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>out_channels <span class="token operator">=</span> out_planes        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_planes<span class="token punctuation">,</span> out_planes<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span>padding<span class="token punctuation">,</span> dilation<span class="token operator">=</span>dilation<span class="token punctuation">,</span> groups<span class="token operator">=</span>groups<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>out_planes<span class="token punctuation">,</span>eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> affine<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">if</span> bn <span class="token keyword">else</span> None        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> relu <span class="token keyword">else</span> None    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bn <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>relu <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>            x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token comment" spellcheck="true">#展平层</span><span class="token keyword">class</span> <span class="token class-name">Flatten</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#通道注意</span><span class="token keyword">class</span> <span class="token class-name">ChannelGate</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> gate_channels<span class="token punctuation">,</span> reduction_ratio<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> pool_types<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'avg'</span><span class="token punctuation">,</span> <span class="token string">'max'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>ChannelGate<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>gate_channels <span class="token operator">=</span> gate_channels        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>gate_channels<span class="token punctuation">,</span> gate_channels <span class="token operator">//</span> reduction_ratio<span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>gate_channels <span class="token operator">//</span> reduction_ratio<span class="token punctuation">,</span> gate_channels<span class="token punctuation">)</span>            <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool_types <span class="token operator">=</span> pool_types    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        channel_att_sum <span class="token operator">=</span> None        <span class="token keyword">for</span> pool_type <span class="token keyword">in</span> self<span class="token punctuation">.</span>pool_types<span class="token punctuation">:</span>            <span class="token keyword">if</span> pool_type<span class="token operator">==</span><span class="token string">'avg'</span><span class="token punctuation">:</span>                avg_pool <span class="token operator">=</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span> x<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                channel_att_raw <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span> avg_pool <span class="token punctuation">)</span>            <span class="token keyword">elif</span> pool_type<span class="token operator">==</span><span class="token string">'max'</span><span class="token punctuation">:</span>                max_pool <span class="token operator">=</span> F<span class="token punctuation">.</span>max_pool2d<span class="token punctuation">(</span> x<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                channel_att_raw <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span> max_pool <span class="token punctuation">)</span>            <span class="token keyword">elif</span> pool_type<span class="token operator">==</span><span class="token string">'lp'</span><span class="token punctuation">:</span>                lp_pool <span class="token operator">=</span> F<span class="token punctuation">.</span>lp_pool2d<span class="token punctuation">(</span> x<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                channel_att_raw <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span> lp_pool <span class="token punctuation">)</span>            <span class="token keyword">elif</span> pool_type<span class="token operator">==</span><span class="token string">'lse'</span><span class="token punctuation">:</span>                <span class="token comment" spellcheck="true"># LSE pool only</span>                lse_pool <span class="token operator">=</span> logsumexp_2d<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                channel_att_raw <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span> lse_pool <span class="token punctuation">)</span>            <span class="token keyword">if</span> channel_att_sum <span class="token keyword">is</span> None<span class="token punctuation">:</span>                channel_att_sum <span class="token operator">=</span> channel_att_raw            <span class="token keyword">else</span><span class="token punctuation">:</span>                channel_att_sum <span class="token operator">=</span> channel_att_sum <span class="token operator">+</span> channel_att_raw        scale <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span> channel_att_sum <span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x <span class="token operator">*</span> scale<span class="token keyword">def</span> <span class="token function">logsumexp_2d</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>    tensor_flatten <span class="token operator">=</span> tensor<span class="token punctuation">.</span>view<span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    s<span class="token punctuation">,</span> _ <span class="token operator">=</span> torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>tensor_flatten<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    outputs <span class="token operator">=</span> s <span class="token operator">+</span> <span class="token punctuation">(</span>tensor_flatten <span class="token operator">-</span> s<span class="token punctuation">)</span><span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>sum<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> outputs<span class="token keyword">class</span> <span class="token class-name">ChannelPool</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">)</span><span class="token comment" spellcheck="true">#空间注意力部分</span><span class="token keyword">class</span> <span class="token class-name">SpatialGate</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>SpatialGate<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        kernel_size <span class="token operator">=</span> <span class="token number">7</span>        self<span class="token punctuation">.</span>compress <span class="token operator">=</span> ChannelPool<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>spatial <span class="token operator">=</span> BasicConv<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span>kernel_size<span class="token number">-1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> relu<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x_compress <span class="token operator">=</span> self<span class="token punctuation">.</span>compress<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x_out <span class="token operator">=</span> self<span class="token punctuation">.</span>spatial<span class="token punctuation">(</span>x_compress<span class="token punctuation">)</span>        scale <span class="token operator">=</span> F<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>x_out<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># broadcasting</span>        <span class="token keyword">return</span> x <span class="token operator">*</span> scale<span class="token keyword">class</span> <span class="token class-name">CBAM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> gate_channels<span class="token punctuation">,</span> reduction_ratio<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> pool_types<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'avg'</span><span class="token punctuation">,</span> <span class="token string">'max'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> no_spatial<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>CBAM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>ChannelGate <span class="token operator">=</span> ChannelGate<span class="token punctuation">(</span>gate_channels<span class="token punctuation">,</span> reduction_ratio<span class="token punctuation">,</span> pool_types<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>no_spatial<span class="token operator">=</span>no_spatial        <span class="token keyword">if</span> <span class="token operator">not</span> no_spatial<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>SpatialGate <span class="token operator">=</span> SpatialGate<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x_out <span class="token operator">=</span> self<span class="token punctuation">.</span>ChannelGate<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>no_spatial<span class="token punctuation">:</span>            x_out <span class="token operator">=</span> self<span class="token punctuation">.</span>SpatialGate<span class="token punctuation">(</span>x_out<span class="token punctuation">)</span>        <span class="token keyword">return</span> x_out</code></pre><p>放在模型中的位置<br><img src="https://img-blog.csdnimg.cn/c03140899eb54767bb2280596f9d2fd2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="Coordinate-Attention"><a href="#Coordinate-Attention" class="headerlink" title="Coordinate Attention"></a>Coordinate Attention</h2><p>发表在CVPR2021<br>结合下面结构图，Coordinate Attention整体思路是，对于输入的特征分别按照h方向和w方向进行池化，也就是变成c×1×w，c×h×1，<br>然后将池化后的特征进行concat拼接，注意不是直接拼接，先将维度调整一样。因为维度不一样，直接拼接会有广播机制，<br>合并后进行1×1的卷积等一系列操作，此时卷积通道数变为原来的1/r，<br>然后再分开，分别在不同的方向上进行sigmoid得到系数，然后相乘。</p><p>😂整个结构就是这样，确实很玄学，但是有效，而且故事讲得好.<br><img src="https://img-blog.csdnimg.cn/d181bed69bad4bb7b1111dfdf1429e42.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>该模块整体思路如图所示</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> math<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">class</span> <span class="token class-name">h_sigmoid</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>h_sigmoid<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span>inplace<span class="token operator">=</span>inplace<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x <span class="token operator">+</span> <span class="token number">3</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">6</span><span class="token keyword">class</span> <span class="token class-name">h_swish</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>h_swish<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> h_sigmoid<span class="token punctuation">(</span>inplace<span class="token operator">=</span>inplace<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x <span class="token operator">*</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">CoordAtt</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inp<span class="token punctuation">,</span> oup<span class="token punctuation">,</span> reduction<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>CoordAtt<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool_h <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span>None<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool_w <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span><span class="token punctuation">)</span>        mip <span class="token operator">=</span> max<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> inp <span class="token operator">//</span> reduction<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>inp<span class="token punctuation">,</span> mip<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>mip<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>act <span class="token operator">=</span> h_swish<span class="token punctuation">(</span><span class="token punctuation">)</span>                self<span class="token punctuation">.</span>conv_h <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mip<span class="token punctuation">,</span> oup<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv_w <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mip<span class="token punctuation">,</span> oup<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>            <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        identity <span class="token operator">=</span> x                n<span class="token punctuation">,</span>c<span class="token punctuation">,</span>h<span class="token punctuation">,</span>w <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>        x_h <span class="token operator">=</span> self<span class="token punctuation">.</span>pool_h<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x_w <span class="token operator">=</span> self<span class="token punctuation">.</span>pool_w<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        y <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>x_h<span class="token punctuation">,</span> x_w<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>act<span class="token punctuation">(</span>y<span class="token punctuation">)</span>                 x_h<span class="token punctuation">,</span> x_w <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>y<span class="token punctuation">,</span> <span class="token punctuation">[</span>h<span class="token punctuation">,</span> w<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>        x_w <span class="token operator">=</span> x_w<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        a_h <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_h<span class="token punctuation">(</span>x_h<span class="token punctuation">)</span><span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        a_w <span class="token operator">=</span> self<span class="token punctuation">.</span>conv_w<span class="token punctuation">(</span>x_w<span class="token punctuation">)</span><span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        out <span class="token operator">=</span> identity <span class="token operator">*</span> a_w <span class="token operator">*</span> a_h        <span class="token keyword">return</span> out</code></pre><p>接下来看一下具体的如何使用：直接加入到残差块里面</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">InvertedResidual</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inp<span class="token punctuation">,</span> oup<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> expand_ratio<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>InvertedResidual<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">assert</span> stride <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>        hidden_dim <span class="token operator">=</span> round<span class="token punctuation">(</span>inp <span class="token operator">*</span> expand_ratio<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>identity <span class="token operator">=</span> stride <span class="token operator">==</span> <span class="token number">1</span> <span class="token operator">and</span> inp <span class="token operator">==</span> oup        <span class="token keyword">if</span> expand_ratio <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>                <span class="token comment" spellcheck="true"># dw</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>hidden_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token comment" spellcheck="true"># pw-linear</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> oup<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>oup<span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>                <span class="token comment" spellcheck="true"># pw</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>inp<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token comment" spellcheck="true"># dw</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> groups<span class="token operator">=</span>hidden_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>ReLU6<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token comment" spellcheck="true"># coordinate attention</span>                CoordAtt<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token comment" spellcheck="true"># pw-linear</span>                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> oup<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>oup<span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>identity<span class="token punctuation">:</span>            <span class="token keyword">return</span> x <span class="token operator">+</span> y        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> y</code></pre><h2 id="ECA"><a href="#ECA" class="headerlink" title="ECA"></a>ECA</h2><p>代码来自仓库：<a href="https://github.com/BangguWu/ECANet[%E6%B7%BB%E5%8A%A0%E9%93%BE%E6%8E%A5%E6%8F%8F%E8%BF%B0](https://github.com/BangguWu/ECANet)">https://github.com/BangguWu/ECANet[添加链接描述](https://github.com/BangguWu/ECANet)</a></p><p><img src="https://img-blog.csdnimg.cn/e6b364fac8cc464e8113d2edb179650f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZC05aSn54Ku,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>基本思想：se模块是给每一个通道一个权重，也就是根据当前通道的特征，给出一个权值，而se中全连接层的通道却由大变小再变大，特征通道变小之后，原来通道数发生边，不再具有每个通道原有的特征。因此提出eca（不知道理解的对不对！）<br>首先将输入的特征通过全局平均池化，然后利用1d卷积进行特征提取，实现跨通道的交互。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parameter <span class="token keyword">import</span> Parameter<span class="token keyword">class</span> <span class="token class-name">eca_layer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Constructs a ECA module.    Args:        channel: Number of channels of the input feature map        k_size: Adaptive selection of kernel size    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> k_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>eca_layer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>avg_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span>k_size<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token punctuation">(</span>k_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>         self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># feature descriptor on the global spatial information</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Two different branches of ECA module</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>y<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Multi-scale information fusion</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        <span class="token keyword">return</span> x <span class="token operator">*</span> y<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        </code></pre><p>后续有时间慢慢补充</p><h2 id="BAM"><a href="#BAM" class="headerlink" title="BAM"></a>BAM</h2><h2 id="RBF（感受野模块）"><a href="#RBF（感受野模块）" class="headerlink" title="RBF（感受野模块）"></a>RBF（感受野模块）</h2>]]></content>
      
      
      
        <tags>
            
            <tag> cv </tag>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
